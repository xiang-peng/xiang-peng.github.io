<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo_miccall.png"/>
	 <link rel="shortcut icon" href="/img/logo_miccall.png">
	
			
    <title>
    Xiang Peng
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    
    	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>

			    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_solarized.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">XIANG PENG</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/网络结构分析/">网络结构分析</a>
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        <li class="active">
	            <a href="#s1">归档</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="archive-link" href="/archives/2017/07/">July 2017</a>
	                    </ul>
	        </li>
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="简历">
		                简历
		            </a>
		        </li>
		        
		        <li>
		            <a href="/group/" title="团队">
		                团队
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="图库">
		                图库
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="标签">
		                标签
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
		            
		                <li><a href="https://github.com/ailib" class="icon fa-github"><span class="label">GitHub</span></a></li>
		            
		            
		            
		            
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(http://chuantu.biz/t6/1/1502529484x3528195284.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >ResNet网络分析</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <p><strong>论文地址</strong> <a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="external">Deep Residual Learning for Image Recognition</a></p>
<ul>
<li><strong>ResNet</strong> —MSRA何凯明团队的Residual Networks，在2015年ImageNet上大放异彩，在ImageNet的classification、detection、localization以及COCO的detection和segmentation上均斩获了第一名的成绩，而且Deep Residual Learning for Image Recognition也获得了CVPR2016的best paper，实在是实至名归。就让我们来观摩大神的这篇上乘之作。</li>
</ul>
<h2 id="ResNet要解决的问题"><a href="#ResNet要解决的问题" class="headerlink" title="ResNet要解决的问题"></a>ResNet要解决的问题</h2><p>首先一个问题是 vanishing/exploding gradients，即梯度的消失或发散。这就导致训练难以收敛。但是随着 normalized initialization and intermediate normalization layers的提出，解决了这个问题。</p>
<p>当收敛问题解决后，又一个问题暴露出来：随着网络深度的增加，系统精度得到饱和之后，迅速的下滑。让人意外的是这个性能下降不是过拟合导致的。对一个合适深度的模型加入额外的层数导致训练误差变大。如下图所示：<br><img src="/2017/07/01/ResNet/1.png" alt="1.png" title=""><br>现在浅层的网络（shallower network）无法明显提升网络的识别效果了，所以现在要解决的问题就是怎样在加深网络的情况下又解决训练误差变大的问题。如果我们加入额外的 层只是一个 identity mapping，那么随着深度的增加，训练误差并没有随之增加。所以我们认为可能存在另一种构建方法，随着深度的增加，训练误差不会增加，只是我们没有找到该方法而已。</p>
<h2 id="ResNet的解决方案"><a href="#ResNet的解决方案" class="headerlink" title="ResNet的解决方案"></a>ResNet的解决方案</h2><p>ResNet引入了残差网络结构（residual network），通过残差网络，可以把网络层弄的很深，据说现在达到了1000多层，最终的网络分类的效果也是非常好，残差网络的基本结构如下图所示 :<br><img src="/2017/07/01/ResNet/2.png" alt="2.png" title=""><br>即增加一个identity mapping（恒等映射），将原始所需要学的函数H(x)转换成F(x)+x，而作者认为这两种表达的效果相同，但是优化的难度却并不相同，作者假设F(x)的优化 会比H(x)简单的多。这一想法也是源于图像处理中的残差向量编码，通过一个reformulation，将一个问题分解成多个尺度直接的残差问题，能够很好的起到优化训练的效果。</p>
<p>这里我们首先求取残差映射 F(x):= H(x)-x，那么原先的映射就是 F(x)+x。尽管这两个映射应该都可以近似理论真值映射 the desired functions (as hypothesized)，但是它俩的学习难度是不一样的。</p>
<p>这种改写启发于 图1中性能退化问题违反直觉的现象。正如前言所说，如果增加的层数可以构建为一个 identity mappings，那么增加层数后的网络训练误差应该不会增加，与没增加之前相比较。性能退化问题暗示多个非线性网络层用于近似identity mappings 可能有困难。使用残差学习改写问题之后，如果identity mappings 是最优的，那么优化问题变得很简单，直接将多层非线性网络参数趋0。</p>
<p>实际中，identity mappings 不太可能是最优的，但是上述改写问题可能对问题提供有效的预先处理 (provide reasonable preconditioning)。如果最优函数接近identity mappings，那么优化将会变得容易些。 实验证明该思路是对的。</p>
<p>这个Residual block通过shortcut connection实现，通过shortcut将这个block的输入和输出进行一个element-wise的加叠，这个简单的加法并不会给网络增加额外的参数和计算量，同时却可以大大增加模型的训练速度、提高训练效果，并且当模型的层数加深时，这个简单的结构能够很好的解决退化问题。</p>
<h3 id="Network-Architectures"><a href="#Network-Architectures" class="headerlink" title="Network Architectures"></a>Network Architectures</h3><img src="/2017/07/01/ResNet/3.png" alt="3.png" title="">
<p>首先构建了一个18层和一个34层的plain网络，即将所有层进行简单的铺叠，然后构建了一个18层和一个34层的residual网络，仅仅是在plain上插入了shortcut，而且这两个网络的参数量、计算量相同，并且和之前有很好效果的VGG-19相比，计算量要小很多。（36亿FLOPs VS 196亿FLOPs，FLOPs即每秒浮点运算次数。）这也是作者反复强调的地方，也是这个模型最大的优势所在。</p>
<p>Plain Network 主要是受 VGG 网络启发，主要采用3*3滤波器，遵循两个设计原则：1）对于相同输出特征图尺寸，卷积层有相同个数的滤波器，2）如果特征图尺寸缩小一半，滤波器个数加倍以保持每个层的计算复杂度。通过步长为2的卷积来进行降采样。一共34个权重层。需要指出,这个网络与VGG相比，滤波器要少，复杂度要小。</p>
<h2 id="对ResNet的解读"><a href="#对ResNet的解读" class="headerlink" title="对ResNet的解读"></a>对ResNet的解读</h2><p>基本的残差网络其实可以从另一个角度来理解，如下图所示： </p>
<img src="/2017/07/01/ResNet/9.png" alt="9.png" title="">
<p>残差网络单元其中可以分解成右图的形式，从图中可以看出，残差网络其实是由多种路径组合的一个网络，直白了说，残差网络其实是很多并行子网络的组合，整个残差网络其实相当于一个多人投票系统（Ensembling）。下面来说明为什么可以这样理解</p>
<h3 id="删除网络的一部分"><a href="#删除网络的一部分" class="headerlink" title="删除网络的一部分"></a>删除网络的一部分</h3><p>如果把残差网络理解成一个Ensambling系统，那么网络的一部分就相当于少一些投票的人，如果只是删除一个基本的残差单元，对最后的分类结果应该影响很小；而最后的分类错误率应该适合删除的残差单元的个数成正比的，论文里的结论也印证了这个猜测。<br>下图是比较VGG和ResNet分别删除一层网络的分类错误率变化 </p>
<img src="/2017/07/01/ResNet/10.png" alt="10.png" title="">
<p>下图是ResNet分类错误率和删除的基本残差网络单元个数的关系 </p>
<img src="/2017/07/01/ResNet/11.png" alt="11.png" title="">
<h3 id="ResNet的真面目"><a href="#ResNet的真面目" class="headerlink" title="ResNet的真面目"></a>ResNet的真面目</h3><p>ResNet的确可以做到很深，但是从上面的介绍可以看出，网络很深的路径其实很少，大部分的网络路径其实都集中在中间的路径长度上，如下图所示： </p>
<img src="/2017/07/01/ResNet/12.png" alt="12.png" title="">
<p>从这可以看出其实ResNet是由大多数中度网络和一小部分浅度网络和深度网络组成的，说明虽然表面上ResNet网络很深，但是其实起实际作用的网络层数并没有很深，我们能来进一步阐述这个问题，我们知道网络越深，梯度就越小，如下图所示 ：</p>
<img src="/2017/07/01/ResNet/13.png" alt="13.png" title="">
<p>而通过各个路径长度上包含的网络数乘以每个路径的梯度值，我们可以得到ResNet真正起作用的网络是什么样的，如下图所示 ：</p>
<img src="/2017/07/01/ResNet/14.png" alt="14.png" title="">
<p>我们可以看出大多数的梯度其实都集中在中间的路径上，论文里称为effective path。<br>从这可以看出其实ResNet只是表面上看起来很深，事实上网络却很浅。<br>所示ResNet真的解决了深度网络的梯度消失的问题了吗？似乎没有，ResNet其实就是一个多人投票系统。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>1.  Cifar10</strong><br><img src="/2017/07/01/ResNet/4.png" alt="4.png" title=""></p>
<p><strong>2.  ImageNet</strong><br>针对 ImageNet网络的实现，图像以较小的边缩放至[256,480]，这样便于 scale augmentation，然后从中随机裁出 224*224。<br><img src="/2017/07/01/ResNet/5.png" alt="5.png" title=""></p>
<p>在plain上观测到明显的退化现象，而且ResNet上不仅没有退化，34层网络的效果反而比18层的更好，而且不仅如此，ResNet的收敛速度比plain的要快得多。</p>
<p><strong>对于shortcut的方式，作者提出了三个选项：</strong></p>
<p>A. 使用恒等映射，如果residual block的输入输出维度不一致，对增加的维度用0来填充；</p>
<p>B. 在block输入输出维度一致时使用恒等映射，不一致时使用线性投影以保证维度一致；</p>
<p>C. 对于所有的block均使用线性投影。</p>
<p>对这三个选项都进行了实验，发现虽然C的效果好于B的效果好于A的效果，但是差距很小，因此线性投影并不是必需的，而使用0填充时，可以保证模型的复杂度最低，这对于更深的网络是更加有利的。</p>
<p>进一步实验，作者又提出了deeper的residual block：</p>
<img src="/2017/07/01/ResNet/8.png" alt="8.png" title="">
<h2 id="计算时间和层数"><a href="#计算时间和层数" class="headerlink" title="计算时间和层数"></a>计算时间和层数</h2><img src="/2017/07/01/ResNet/6.png" alt="6.png" title="">
<img src="/2017/07/01/ResNet/7.png" alt="7.png" title="">
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding:utf-8</span></div><div class="line"><span class="comment">#导入对应的库</span></div><div class="line"><span class="keyword">import</span> collections</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</div><div class="line">slim = tf.contrib.slim</div><div class="line"></div><div class="line"><span class="comment">#定义了一个block类</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span><span class="params">(collections.namedtuple<span class="params">(<span class="string">'Bolck'</span>, [<span class="string">'scope'</span>, <span class="string">'init_fn'</span>, <span class="string">'args'</span>])</span>)</span>:</span></div><div class="line">    <span class="string">'A named tuple describing a ResNet block.'</span></div><div class="line"></div><div class="line"><span class="comment">#定义了一个下采样函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsample</span><span class="params">(inputs, factor, scope = None)</span>:</span></div><div class="line">    <span class="keyword">if</span> factor == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> inputs</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> slim.max_pool2d(inputs, [<span class="number">1</span>, <span class="number">1</span>], stride = factor, scope = scope)</div><div class="line"></div><div class="line"><span class="comment">#定义conv2d_same函数创建卷积层</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d_same</span><span class="params">(inputs, num_outputs, kernel_size, stride, scope = None)</span>:</span></div><div class="line">    <span class="keyword">if</span> stride == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> slim.conv2d(inputs, num_outputs, kernel_size, stride = <span class="number">1</span>, padding = <span class="string">'SAME'</span>, scope = scope)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        pad_total = kernel_size - <span class="number">1</span></div><div class="line">        pad_beg = pad_total // <span class="number">2</span></div><div class="line">        pad_end = pad_total - pad_beg</div><div class="line">        inputs = tf.pad(inputs, [[<span class="number">0</span>, <span class="number">0</span>], [pad_beg, pad_end], [pad_beg, pad_end], [<span class="number">0</span>, <span class="number">0</span>]])</div><div class="line">        <span class="keyword">return</span> slim.conv2d(inputs, num_outputs, kernel_size, stride = stride, padding = <span class="string">'VALID'</span>, scope = scope)</div><div class="line"></div><div class="line"><span class="comment">#定义堆叠blocks的函数</span></div><div class="line"><span class="comment">#@这个符号用于装饰器中，用于修饰一个函数，把被修饰的函数作为参数传递给装饰器</span></div><div class="line"><span class="meta">@slim.add_arg_scope</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stack_blocks_dense</span><span class="params">(net, blocks, outputs_collections = None)</span>:</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> blocks:</div><div class="line">        <span class="keyword">with</span> tf.variable_scope(block.scope, <span class="string">'block'</span>, [net]) <span class="keyword">as</span> sc:</div><div class="line">            <span class="keyword">for</span> i, unit <span class="keyword">in</span> enumerate(block.args):</div><div class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">'unit_%d'</span> %(i + <span class="number">1</span>), values = [net]):</div><div class="line">                    unit_depth, unit_depth_bottleneck, unit_stride = unit </div><div class="line">                    net = block.unit_fn(net, depth = unit_depth,</div><div class="line">                                    unit_depth_bottleneck = unit_depth_bottleneck,</div><div class="line">                                    stride = unit_stride)</div><div class="line">                    net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> net</div><div class="line"></div><div class="line"><span class="comment">#创建ResNet通用的arg_scope</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_arg_scope</span><span class="params">(is_training = True,</span></span></div><div class="line"><span class="function"><span class="params">                    weight_decay = <span class="number">0.0001</span>,</span></span></div><div class="line"><span class="function"><span class="params">                    batch_norm_decay = <span class="number">0.997</span>,</span></span></div><div class="line"><span class="function"><span class="params">                    batch_norm_epsilon = <span class="number">1e-5</span>,</span></span></div><div class="line"><span class="function"><span class="params">                    batch_norm_scale = True)</span>:</span></div><div class="line"></div><div class="line">    batch_norm_params = &#123;</div><div class="line">        <span class="string">'is_training'</span>: is_training,</div><div class="line">        <span class="string">'decay'</span>: batch_norm_decay,</div><div class="line">        <span class="string">'epsilon'</span>: batch_norm_epsilon,</div><div class="line">        <span class="string">'scale'</span>: batch_norm_scale,</div><div class="line">        <span class="string">'updates_collections'</span>: tf.GraphKeys.UPDATE_OPS,</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">with</span> slim.arg_scope(</div><div class="line">            [slim.conv2d],</div><div class="line">            weights_regularizer = slim.l2_regularizer(weight_decay),</div><div class="line">            weights_initializer = slim.variance_scaling_initializer(),</div><div class="line">            activation_fn = tf.nn.relu,</div><div class="line">            normalizer_fn = slim.batch_norm,</div><div class="line">            normalizer_params = batch_norm_params):</div><div class="line">        <span class="keyword">with</span> slim.arg_scope([slim.batch_norm], **batch_norm_params):</div><div class="line">            <span class="keyword">with</span> slim.arg_scope([slim.max_pool2d], padding = <span class="string">'SAME'</span>) <span class="keyword">as</span> arg_sc:</div><div class="line">                <span class="keyword">return</span> arg_sc</div><div class="line"></div><div class="line"><span class="comment">#定义bottleneck残差学习单元</span></div><div class="line"><span class="meta">@slim.add_arg_scope</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bottleneck</span><span class="params">(inputs, depth, depth_bottleneck, stride, outputs_collections = None, scope = None)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(scope, <span class="string">'bottleneck_v2'</span>, [inputs]) <span class="keyword">as</span> sc:</div><div class="line">        depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank = <span class="number">4</span>)</div><div class="line">        preact = slim.batch_norm(inputs, activation_fn = tf.nn.relu, scope = <span class="string">'preact'</span>)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> depth == depth_in:</div><div class="line">            shortcut = subsample(inputs, stride, <span class="string">'shortcut'</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            shortcut = slim.conv2d(preact, depth, [<span class="number">1</span>, <span class="number">1</span>], stride = stride, normalizer_fn = <span class="keyword">None</span>, activation_fn = <span class="keyword">None</span>, scope = <span class="string">'shortcut'</span>)    </div><div class="line">            residual = slim.conv2d(preact, depth_bottleneck, [<span class="number">1</span>, <span class="number">1</span>], stride = <span class="number">1</span>, scope = <span class="string">'conv1'</span>)</div><div class="line">            residual = conv2d_same(residual, depth_bottleneck, <span class="number">3</span>, stride, scope = <span class="string">'conv2'</span>)</div><div class="line">            residual = slim.conv2d(residual, depth, [<span class="number">1</span>, <span class="number">1</span>], stride = <span class="number">1</span>, normalizer_fn = <span class="keyword">None</span>, activation_fn = <span class="keyword">None</span>, scope = <span class="string">'conv3'</span>)    </div><div class="line"></div><div class="line">        output = shortcut + residual</div><div class="line"></div><div class="line">        <span class="keyword">return</span> slim.utils.collect_named_outputs(outputs_collections, sc.name, output)</div><div class="line"></div><div class="line"><span class="comment">#定义生成ResNet V2主函数</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_v2</span><span class="params">(inputs, blocks, num_classes = None, global_pool = True, include_root_block = True, reuse = None, scope = None)</span>:</span>    </div><div class="line">    <span class="keyword">with</span> tf.variable_scope(scope, <span class="string">'resnet_v2'</span>, [inputs], reuse = reuse) <span class="keyword">as</span> sc:</div><div class="line">        end_points_collection = sc.original_name_scope + <span class="string">'_end_points'</span></div><div class="line">        <span class="keyword">with</span> slim.arg_scope([slim.conv2d, bottleneck, stack_blocks_dense], outputs_collections = end_points_collection):    </div><div class="line">            net = inputs</div><div class="line"></div><div class="line">        <span class="keyword">if</span> include_root_block:</div><div class="line">            <span class="keyword">with</span> slim.arg_scope([slim.conv2d], activation_fn = <span class="keyword">None</span>, normalizer_fn = <span class="keyword">None</span>):</div><div class="line">                net = conv2d_same(net, <span class="number">64</span>, <span class="number">7</span>, stride = <span class="number">2</span>, scope = <span class="string">'conv1'</span>)</div><div class="line">            net = slim.max_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>], stride = <span class="number">2</span>, scope = <span class="string">'pool1'</span>)</div><div class="line">            net = slim.batch_norm(net, activation_fn = tf.nn.relu, scope = <span class="string">'postnorm'</span>)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> global_pool:</div><div class="line">            net = tf.reduce_mean(net, [<span class="number">1</span>, <span class="number">2</span>], name = <span class="string">'pool5'</span>, keep_dims = <span class="keyword">True</span>)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            net = slim.conv2d(net, num_classes, [<span class="number">1</span>, <span class="number">1</span>], activation_fn = <span class="keyword">None</span>, normalizer_fn = <span class="keyword">None</span>, scope = <span class="string">'logits'</span>)    </div><div class="line">            end_points = slim.utils.convert_collection_to_dict(end_points_collection)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">                end_points[<span class="string">'predictions'</span>] = slim.softmax(net, scope = <span class="string">'predictions'</span>)</div><div class="line"></div><div class="line">            <span class="keyword">return</span> net, end_points</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#定义50层的ResNet   </span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_v2_50</span><span class="params">(inputs, num_classes = None, global_pool = True, reuse = None, scope = <span class="string">'resnet_v2_50'</span>)</span>:</span>    </div><div class="line">    blocks = [</div><div class="line">        Block(<span class="string">'block1'</span>, bottleneck, [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">1</span>)] * <span class="number">2</span> + [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">2</span>)]),</div><div class="line">        Block(<span class="string">'block2'</span>, bottleneck, [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">1</span>)] * <span class="number">3</span> + [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">2</span>)]),</div><div class="line">        Block(<span class="string">'block3'</span>, bottleneck, [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">1</span>)] * <span class="number">5</span> + [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">2</span>)]),</div><div class="line">        Block(<span class="string">'block4'</span>, bottleneck, [(<span class="number">2048</span>, <span class="number">1024</span>, <span class="number">1</span>)] * <span class="number">3</span>)]</div><div class="line">    <span class="keyword">return</span> resnet_v2(inputs, blocks, num_classes, global_pool, include_root_block = <span class="keyword">True</span>, reuse = reuse, scope = scope)     </div><div class="line"></div><div class="line"><span class="comment">#定义101层的ResNet  </span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_v2_101</span><span class="params">(inputs, num_classes = None, global_pool = True, reuse = None, scope = <span class="string">'resnet_v2_101'</span>)</span>:</span>     </div><div class="line">    blocks = [</div><div class="line">        Block(<span class="string">'block1'</span>, bottleneck, [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">1</span>)] * <span class="number">2</span> + [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">2</span>)]),</div><div class="line">        Block(<span class="string">'block2'</span>, bottleneck, [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">1</span>)] * <span class="number">3</span> + [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">2</span>)]),</div><div class="line">        Block(<span class="string">'block3'</span>, bottleneck, [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">1</span>)] * <span class="number">22</span> + [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">2</span>)]),</div><div class="line">        Block(<span class="string">'block4'</span>, bottleneck, [(<span class="number">2048</span>, <span class="number">512</span>, <span class="number">1</span>)] * <span class="number">3</span>)]</div><div class="line">    <span class="keyword">return</span> resnet_v2(inputs, blocks, num_classes, global_pool, include_root_block = <span class="keyword">True</span>, reuse = reuse, scope = scope)       </div><div class="line"></div><div class="line"><span class="comment">#定义152层的ResNet  </span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_v2_152</span><span class="params">(inputs, num_classes = None, global_pool = True, reuse = None, scope = <span class="string">'resnet_v2_152'</span>)</span>:</span>      </div><div class="line">    blocks = [</div><div class="line">        Block(<span class="string">'block1'</span>, bottleneck, [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">1</span>)] * <span class="number">2</span> + [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">2</span>)]),</div><div class="line">        Block(<span class="string">'block2'</span>, bottleneck, [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">1</span>)] * <span class="number">7</span> + [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">2</span>)]),</div><div class="line">        Block(<span class="string">'block3'</span>, bottleneck, [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">1</span>)] * <span class="number">35</span> + [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">2</span>)]),</div><div class="line">        Block(<span class="string">'block4'</span>, bottleneck, [(<span class="number">2048</span>, <span class="number">512</span>, <span class="number">1</span>)] * <span class="number">3</span>)]</div><div class="line">    <span class="keyword">return</span> resnet_v2(inputs, blocks, num_classes, global_pool, include_root_block = <span class="keyword">True</span>, reuse = reuse, scope = scope)        </div><div class="line"></div><div class="line"><span class="comment">#定义200层的ResNet</span></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">resnet_v2_200</span><span class="params">(inputs, num_classes = None, global_pool = True, reuse = None, scope = <span class="string">'resnet_v2_200'</span>)</span>:</span>      </div><div class="line">    blocks = [</div><div class="line">        Block(<span class="string">'block1'</span>, bottleneck, [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">1</span>)] * <span class="number">2</span> + [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">2</span>)]),</div><div class="line">        Block(<span class="string">'block2'</span>, bottleneck, [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">1</span>)] * <span class="number">23</span> + [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">2</span>)]),</div><div class="line">        Block(<span class="string">'block3'</span>, bottleneck, [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">1</span>)] * <span class="number">35</span> + [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">2</span>)]),</div><div class="line">        Block(<span class="string">'block4'</span>, bottleneck, [(<span class="number">2048</span>, <span class="number">512</span>, <span class="number">1</span>)] * <span class="number">3</span>)]</div><div class="line">    <span class="keyword">return</span> resnet_v2(inputs, blocks, num_classes, global_pool, include_root_block = <span class="keyword">True</span>, reuse = reuse, scope = scope)              </div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_tensorflow_run</span><span class="params">(session, target, info_string)</span>:</span></div><div class="line">    num_steps_burn_in = <span class="number">10</span></div><div class="line">    total_duration = <span class="number">0.0</span></div><div class="line">    total_duration_squared = <span class="number">0.0</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_batches + num_steps_burn_in):</div><div class="line">        start_time = time.time()</div><div class="line">        _ = session.run(target)</div><div class="line">        duration = time.time() - start_time</div><div class="line">        <span class="keyword">if</span> i &gt;= num_steps_burn_in:</div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> i % <span class="number">10</span>:</div><div class="line">                print(<span class="string">'%s: step %d, duration = %.3f'</span> %(datetime.now(), i - num_steps_burn_in, duration))            </div><div class="line">            total_duration += duration</div><div class="line">            total_duration_squared += duration * duration</div><div class="line"></div><div class="line">    mn = total_duration / num_batches</div><div class="line">    vr = total_duration_squared / num_batches - mn * mn</div><div class="line">    sd = math.sqrt(vr)</div><div class="line">    print(<span class="string">'%s: %s across %d steps, %.3f +/- %.3f sec / batch'</span> %(datetime.now(), info_string, num_batches, mn, sd))             </div><div class="line"></div><div class="line">batch_size = <span class="number">32</span></div><div class="line">height, width = <span class="number">224</span>, <span class="number">224</span></div><div class="line">inputs = tf.random_uniform((batch_size, height, width, <span class="number">3</span>))</div><div class="line"><span class="keyword">with</span> slim.arg_scope(resnet_arg_scope(is_training = <span class="keyword">False</span>)):</div><div class="line">    net, end_points = resnet_v2_152(inputs, <span class="number">1000</span>)</div><div class="line"></div><div class="line">init = tf.global_variables_initializer()</div><div class="line">sess = tf.Session()</div><div class="line">sess.run(init)</div><div class="line">num_batches = <span class="number">100</span></div><div class="line">time_tensorflow_run(sess, net, <span class="string">"Forward"</span>)</div></pre></td></tr></table></figure>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'thank you'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://yoursite.com/2017/07/01/ResNet/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://yoursite.com/2017/07/01/ResNet/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//thank you.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
            	<span id="busuanzi_container_site_pv">2017总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
</body>



 	
</html>
